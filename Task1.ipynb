{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identification Task \n",
    "\n",
    "## What we need\n",
    "\n",
    "We need a function called check_msm_identification that it easy to use and performs the identification check described in [this paper](https://arxiv.org/pdf/1907.13093.pdf). The different variants (e.g. different methods of sampling uniformly from likelihood level sets) can be selected via the optional arguments of check_ml_identification.  The output will either be a dictionary (if it is a small set of outputs that every user will want) or a results object similar to the result of estimate_ml (if there are many different test statistics).\n",
    "\n",
    "## Task 1: Planning\n",
    "\n",
    "- Write down which model specific inputs a user has to supply in order to do an identification check. The names should be aligned with estimate_ml where possible. It will definitely be a likelihood function and a result of estimate_msm but there might be more. \n",
    "- Write down which kinds of outputs a user will get, what they mean and how they should be visualized in a paper (plots, tables, ...). \n",
    "- Write docstrings for check_ml_identification before you actually implement it\n",
    "- Adjust our [simple example](https://estimagic.readthedocs.io/en/stable/getting_started/estimation/first_msm_estimation_with_estimagic.html) such that it has a second variable that can be arbitrarily correlated with x (i.e. add an identification problem)\n",
    "- Start to write a tutorial in a notebook that shows how the new function will be used and what the outputs mean\n",
    "\n",
    "## Remarks\n",
    "\n",
    "- You can for now assume that the model parameters (params) are a 1d numpy array. We talk about making this more flexible later. \n",
    "- The idea behind writing the documentation first is that it lets you focus completely on a user friendly interface and a high level understanding. Also, we will probably ask for changes after you show us your proposed interface. If you had already implemented it, you would have to change it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ncx2\n",
    "from scipy.linalg import sqrtm\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats.qmc as qmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task 1 - interface - inputs and outputs\n",
    "def check_msm_identification():\n",
    "    \"\"\" Do detecting of identification failure in moment condition models. \n",
    "    Performs the identification check as described in Forneron, J. J. (2019).\n",
    "    Introduces the quasi-Jacobian matrix computed as a slope of a linear \n",
    "    approximation of the moments of the estimate of the identified set. It is \n",
    "    asymptotically singular when local or global identification fails and equivalent \n",
    "    to the usual Jacobian matrix which has full rank when model is point and locally \n",
    "    identified.\n",
    "\n",
    "    Args\n",
    "        simulate_moments (callable) – Function that takes as inputs model parameters, \n",
    "            data and potentially other keyword arguments and returns a pytree with \n",
    "            simulated moments.  If the function returns a dict containing the key \n",
    "            \"simulated_moments\" we only use the value corresponding to that key. Other \n",
    "            entries are stored in the log database if you use logging.\n",
    "        simulate_moments_kwargs (dict) – Additional keyword arguments for simulate_moments \n",
    "            with, for example, data on dependent and independent variables from the model \n",
    "            specification.\n",
    "        params (pytree) – A pytree containing the estimated parameters of the model.\n",
    "            Pytrees can be a numpy array, a pandas Series, a DataFrame with “value” column, a \n",
    "            float and any kind of (nested) dictionary or list containing these elements.\n",
    "        weights (str) – One of “diagonal” (default), “identity” or “optimal”.  Note that \n",
    "            “optimal” refers to the asymptotically optimal weighting matrix and is often \n",
    "            not a good choice due to large finite sample bias.\n",
    "        bandwidth (float) - By default is calculated in the form of sqrt(2log(log[n])/n). \n",
    "            Required for the calculation of quasi-jacobian matrix.\n",
    "        kernel (callable) - By default  is the uniform kernel K(U) which is indicator \n",
    "            function for |U|<=1. Required for the calculation of quasi-jacobian matrix.\n",
    "        cutoff (float) - By default is calculated in the form sqrt(2log[n]/n). Required \n",
    "            for identification category selection.\n",
    "        draws (float)  - The number of draws for sampling on level sets. Supposed to be \n",
    "            sufficiently large.\n",
    "        sampling (str) - Methods of sampling uniformly from likelihood level sets. One of \n",
    "            the available options for direct approach using \"sobol\" or \"halton\" sequence \n",
    "            or adaptive sampling by \"population_mc\".\n",
    "        significance (float) - The significance level with default level 5%.\n",
    "        H0 (dict) - Required for subvector inference. For example, b10 = 0.\n",
    "        reparametrization - .\n",
    "        logging (pathlib.Path, str or False) – Path to sqlite3 file (which typically has \n",
    "            the file extension .db. If the file does not exist, it will be created. The \n",
    "            dashboard can only be used when logging is used.\n",
    "        log_options (dict) – Additional keyword arguments to configure the logging.\n",
    "\n",
    "    Returns\n",
    "        dict: The estimated quasi-Jacobian, singular values, identification category, subvestor \n",
    "            inference test output and confidence set.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "    # 1. quasi-Jacobian Matrix\n",
    "    # 1.1 set the integration grid and evaluate the moments on thr grid, select draws on the level set\n",
    "    # 1.2 compute the intercept and slope\n",
    "    # 1.3 compute the variance\n",
    "\n",
    "\n",
    "    # 2. Identification Category Selection\n",
    "    # 2.1 compute singular values\n",
    "    # 2.2 number of values grater than cutoff\n",
    "\n",
    "\n",
    "\n",
    "    # 3. Subvector Inference\n",
    "    # 3.1 test statistic\n",
    "    # 3.2 hypothesis, confidence set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "estimate_msm\n",
    "https://estimagic.readthedocs.io/en/stable/reference_guides/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CALCULATE NECESSARY INPUTS (as in identification_check_with_estimagic.ipynb)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import estimagic as em\n",
    "\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "def simulate_data(params, n_draws, rng,correlation=0.7):\n",
    "\n",
    "    mu = np.array([0.0, 0.0])\n",
    "    var_cov = np.array([\n",
    "            [  1, correlation],\n",
    "            [ correlation,  1],\n",
    "        ])\n",
    "    x = rng.multivariate_normal(mu, var_cov, size=n_draws)\n",
    "    x1 = x[:,0]\n",
    "    x2 = x[:,1]\n",
    "    e = rng.normal(0, params.loc[\"sd\", \"value\"], size=n_draws)\n",
    "    y = params.loc[\"intercept\", \"value\"] + params.loc[\"slope1\", \"value\"] * x1 + params.loc[\"slope2\", \"value\"] + e\n",
    "    return pd.DataFrame({\"y\": y, \"x1\": x1, \"x2\": x2})\n",
    "\n",
    "true_params = pd.DataFrame(\n",
    "    data=[[2, -np.inf], [-1, -np.inf], [-1, -np.inf], [1, 1e-10]],\n",
    "    columns=[\"value\", \"lower_bound\"],\n",
    "    index=[\"intercept\", \"slope1\", \"slope2\", \"sd\"],\n",
    ")\n",
    "\n",
    "true_params = pd.DataFrame(\n",
    "    data=[[2, -np.inf], [-1, -np.inf], [-1, -np.inf], [1, 1e-10]],\n",
    "    columns=[\"value\", \"lower_bound\"],\n",
    "    index=[\"intercept\", \"slope1\", \"slope2\", \"sd\"],\n",
    ")\n",
    "\n",
    "data = simulate_data(true_params, n_draws=100, rng=rng)\n",
    "\n",
    "def calculate_moments(sample):\n",
    "    moments = {\n",
    "        \"y_mean\": sample[\"y\"].mean(),\n",
    "        \"x1_mean\": sample[\"x1\"].mean(),\n",
    "        \"x2_mean\": sample[\"x2\"].mean(),\n",
    "        \"yx1_mean\": (sample[\"y\"] * sample[\"x1\"]).mean(),\n",
    "        \"yx2_mean\": (sample[\"y\"] * sample[\"x2\"]).mean(),\n",
    "        \"y_sqrd_mean\": (sample[\"y\"] ** 2).mean(),\n",
    "        \"x1_sqrd_mean\": (sample[\"x1\"] ** 2).mean(),\n",
    "        \"x2_sqrd_mean\": (sample[\"x1\"] ** 2).mean(),\n",
    "    }\n",
    "    return pd.Series(moments)\n",
    "\n",
    "empirical_moments = calculate_moments(data)\n",
    "\n",
    "\n",
    "def simulate_moments(params, n_draws=10_000, seed=0, data=False):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    sim_data = simulate_data(params, n_draws, rng)\n",
    "    sim_moments = calculate_moments(sim_data)\n",
    "    \n",
    "    if data==False:\n",
    "        return sim_moments\n",
    "    else:\n",
    "        return sim_moments, sim_data\n",
    "\n",
    "moments_cov = em.get_moments_cov(\n",
    "    data, calculate_moments, bootstrap_kwargs={\"n_draws\": 5_000, \"seed\": 0}\n",
    ")\n",
    "\n",
    "start_params = true_params.assign(value=[100, 100, 100, 100])\n",
    "\n",
    "res = em.estimate_msm(\n",
    "    simulate_moments,\n",
    "    empirical_moments,\n",
    "    moments_cov,\n",
    "    start_params,\n",
    "    optimize_options={\"algorithm\":\"scipy_lbfgsb\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from estimagic.estimation.msm_weighting import get_weighting_matrix\n",
    "\n",
    "msm_params = res.params['value'] # estimated coefficient vector\n",
    "n_params = len(msm_params) # number of estimated parameters\n",
    "n = data.shape[0] # number of observations\n",
    "B = 10000 # number of draws\n",
    "\n",
    "\n",
    "sampling = \"sobol\" # for step 2.i)\n",
    "bandwidth = 2 * math.log(math.log(n)) / n # for step 2.i)\n",
    "weights = 'diagonal' # for weighting matrix in step 2.i)\n",
    "kernel = 'uniform' # for step 2.ii) linear approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.i Draws on the level set\n",
    "## Sampling for step 2.i)\n",
    "#### 1. Direct approach\n",
    " - draw values from the space - either randomly or pseudo-randomly (Sobol or Halton)\n",
    " - assign weights proportionally to the bandwidth criterion (indicator function)\n",
    " - drawback - effective sample size can be samll relative to the parameter space; especially when the dimention of \\theta is moderately large\n",
    "\n",
    "#### 2. Adaptive Sampling by Population Monte Carlo\n",
    "- constructing a sequence of proposal distributions with higher acceptance rate\n",
    "- to do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aida4\\Anaconda3\\envs\\assignment_5\\lib\\site-packages\\scipy\\stats\\_qmc.py:1078: UserWarning: The balance properties of Sobol' points require n to be a power of 2.\n",
      "  warnings.warn(\"The balance properties of Sobol' points require\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<=' not supported between instances of 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15896/1519362317.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[1;31m# Select draws on the level set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobjs\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mbandwidth\u001b[0m \u001b[1;31m# why it was objs - min(objs) in the code? according to the paper it should be objs <= bandwidth\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m     \u001b[0mind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m     \u001b[0mgrid_sub\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparams_draws\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<=' not supported between instances of 'list' and 'float'"
     ]
    }
   ],
   "source": [
    "if sampling not in ['random','sobol','halton','population_mc']:\n",
    "    raise NotImplementedError(\"Custom sampling is not yet implemented.\")\n",
    "\n",
    "# direct approach\n",
    "if sampling in ['random','sobol','halton']:\n",
    "    if sampling==\"random\":\n",
    "        random_sequences = np.random.random((B,n_params))\n",
    "        params_draws = [list(msm_params)]*B + 2 * (random_sequences - 1 / 2)\n",
    "\n",
    "    elif sampling==\"sobol\":\n",
    "        sobol_sequences = qmc.Sobol(d = n_params, scramble = True) # d - dimention - number of estimated parameters\n",
    "        sobol_sequences = sobol_sequences.random(n=B) # instead of n=B try power of 2 (for Sobol sequence)\n",
    "        params_draws = [list(msm_params)]*B + 2 * (sobol_sequences - 1 / 2)\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Sampling with Halton sequences is not yet implemented.\")\n",
    "\n",
    "    \n",
    "    # weighting the resulted draws\n",
    "    objs = [np.nan] * B  # Store GMM objective values\n",
    "    moms = [[np.nan] * n_params]  * B # Store sample moments mom\n",
    "    Vs = [np.full([n_params, n_params], np.nan) for i in range(B)] # Store variances V\n",
    "\n",
    "    for b in range(B): # Evaluate the moments on the grid\n",
    "        mm, sim_data = simulate_moments(pd.DataFrame(data = params_draws[b], index = list(msm_params.index), columns = [\"value\"]), n_draws=10_000, seed=0, data=True)\n",
    "        # ???????? cannot use the em.get_moments_cov because the data generated for moments (in simulate_moments) was not saved\n",
    "        V = em.get_moments_cov(sim_data, calculate_moments, bootstrap_kwargs={\"n_draws\": 5_000, \"seed\": 0})\n",
    "        moms[b] = mm\n",
    "        Vs[b] = V\n",
    "        W, internal_weights = get_weighting_matrix( # weighting matrix\n",
    "            moments_cov=V,\n",
    "            method=weights,\n",
    "            empirical_moments=mm,\n",
    "            return_type=\"pytree_and_array\",\n",
    "        )\n",
    "        \n",
    "        objs[b] = np.dot(mm.T, np.linalg.solve(W, mm))\n",
    "        \n",
    "    # Select draws on the level set\n",
    "    ind = np.array(objs) <= bandwidth # why it was objs - min(objs) in the code? according to the paper it should be objs <= bandwidth \n",
    "    ind = [i for i, x in enumerate(ind) if x]\n",
    "    grid_sub = params_draws[ind]\n",
    "    moms_sub = [moms[i] for i in ind]\n",
    "    Vs_sub = [Vs[i] for i in ind] \n",
    "\n",
    "\n",
    "# adaptive sampling\n",
    "elif sampling==\"population_mc\":\n",
    "    raise NotImplementedError(\"Adaptive Sampling by Population Monte Carlo is not yet implemented.\")\n",
    "\n",
    "else:\n",
    "    raise NotImplementedError(\"Custom sequences are not yet implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.ii) Linear approximation\n",
    "## Kernels for step 2.ii)\n",
    " - uniform\n",
    " - Epanchnikov- to do later\n",
    " - cosine - to do later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.34796216, -0.96126163, -0.2366117 ,  1.3666092 ],\n",
       "       [-0.0836938 ,  0.14177115,  0.74147786,  0.90494826],\n",
       "       [-0.13637438, -0.39994483,  0.35566129,  1.63163665],\n",
       "       [ 1.14155937,  0.02938015,  0.27657808,  1.15313479]])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[y_mean         -0.574559\n",
       " x1_mean        -0.006568\n",
       " x2_mean        -0.003578\n",
       " yx1_mean       -0.924803\n",
       " yx2_mean       -0.647135\n",
       " y_sqrd_mean     3.072987\n",
       " x1_sqrd_mean    0.981403\n",
       " x2_sqrd_mean    0.981403\n",
       " dtype: float64,\n",
       " y_mean          0.659304\n",
       " x1_mean        -0.006568\n",
       " x2_mean        -0.003578\n",
       " yx1_mean        0.144577\n",
       " yx2_mean        0.106572\n",
       " y_sqrd_mean     1.274723\n",
       " x1_sqrd_mean    0.981403\n",
       " x2_sqrd_mean    0.981403\n",
       " dtype: float64,\n",
       " y_mean          0.226333\n",
       " x1_mean        -0.006568\n",
       " x2_mean        -0.003578\n",
       " yx1_mean       -0.376346\n",
       " yx2_mean       -0.258246\n",
       " y_sqrd_mean     2.851829\n",
       " x1_sqrd_mean    0.981403\n",
       " x2_sqrd_mean    0.981403\n",
       " dtype: float64,\n",
       " y_mean          1.421067\n",
       " x1_mean        -0.006568\n",
       " x2_mean        -0.003578\n",
       " yx1_mean        0.031959\n",
       " yx2_mean        0.028996\n",
       " y_sqrd_mean     3.348481\n",
       " x1_sqrd_mean    0.981403\n",
       " x2_sqrd_mean    0.981403\n",
       " dtype: float64]"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moms_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# where is the kernel part???\n",
    "# check on how this Chebyshev (minimax) approxiation problem is solved\n",
    "X = np.column_stack([[1] * len(grid_sub), grid_sub])\n",
    "coef = np.linalg.solve(np.dot(X.T, X), np.dot(X.T, moms_sub)) # A_n and B_n #!!!!! not sure whether this is always how the solution is calculated\n",
    "# Here the Kernel K is uniform , re-weigthing is not needed\n",
    "Bn = coef[1: ].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.34796216, -0.96126163, -0.2366117 ,  1.3666092 ],\n",
       "       [ 1.        , -0.0836938 ,  0.14177115,  0.74147786,  0.90494826],\n",
       "       [ 1.        , -0.13637438, -0.39994483,  0.35566129,  1.63163665],\n",
       "       [ 1.        ,  1.14155937,  0.02938015,  0.27657808,  1.15313479]])"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('assignment_5')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4a299800501f871bbb90ae62193ee27158c182734852d8fba03b7f21f11a1c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
